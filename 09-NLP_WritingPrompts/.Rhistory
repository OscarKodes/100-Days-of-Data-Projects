# Examine the topics  --------------
head(prompt_lda_topics)
# turn our LDA into the
# tidy format of the tidyverse packages
prompt_lda_tidy <- tidy(prompt_lda)
# Beta shows how much each term
# contributes to each topic
# NOTE: Typically list 8 terms when topic modelling
top_terms <- prompt_lda_tidy %>%
group_by(topic) %>%
top_n(8, beta) %>%
ungroup() %>%
arrange(topic, -beta)
head(top_terms, 10)
# visualize with ggplot
# NOTE: We want to treat the topics as categorical,
# so turn it to a factor first.
# NOTE: Don't show legend because it's not helpful.
top_terms %>%
mutate(term = reorder(term, beta)) %>%
ggplot(aes(term,
beta,
fill = factor(topic))) +
geom_col(show.legend=FALSE) +
facet_wrap( ~ topic, scales="free") +
coord_flip()
# Remove stop words
my_stops <- c(stopwords("en"),
"just", "can", "like")
prompt_cleaned <- tm_map(prompt_cleaned, removeWords, my_stops)
# Remove white space
prompt_cleaned <- tm_map(prompt_cleaned, stripWhitespace)
# Checking the 3rd document's uncleaned text
prompt_corpus[[3]][1]
# Checking the 3rd document's cleaned text
prompt_cleaned[[3]][1]
#########################################################################
# TOPIC MODELING (DOCUMENT TERM MATRIX)
prompt_dtm <- DocumentTermMatrix(prompt_cleaned)
prompt_dtm
# Sparse entries are 0's, so a specific term does
# not occur in that document.
# There are about 5,853 unique words across our documents.
# They don't appear in every document.
# a little more cleaning to remove empty rows
# keep only indexes of unique words
unique_indexes <- unique(prompt_dtm$i)
prompt_dtm <- prompt_dtm[unique_indexes,]
prompt_dtm
# turn our unique words into the
# tidy format of the tidyverse packages
prompt_dtm_tidy <- tidy(prompt_dtm)
prompt_dtm_tidy
# LDA ANALYSIS ------------------------------------
# k is the number of topics we want
# (like k means clustering?)
k <- 3
# NOTE: Optimal k varies, so you'll need to experiment
# with which k value is best.
prompt_lda <- LDA(prompt_dtm,
k = k,
control = list(seed=42))
prompt_lda
# NOTE: seed keeps randomization consistent
# get the words out of our LDA
prompt_lda_words <- terms(prompt_lda, 5)
prompt_lda_words
# NOTE: at this stage you may want to go back
# and add some stop words to your posts based
# on the results you got from LDA.
# Remove words that don't add any meaning
# to the analysis.
# Save LDA words as CSV --------------
# need a matrix format to be written to csv
prompt_lda_topics <- as.matrix(prompt_lda_words)
# write.csv(prompt_lda_topics,
#           file = paste("prompt_LDA_",
#                        k,
#                        ".csv",
#                        sep = ""))
# NOTE: k variable is helpful here when you run many LDAs
# with different k's
# Examine the topics  --------------
head(prompt_lda_topics)
# turn our LDA into the
# tidy format of the tidyverse packages
prompt_lda_tidy <- tidy(prompt_lda)
# Beta shows how much each term
# contributes to each topic
# NOTE: Typically list 8 terms when topic modelling
top_terms <- prompt_lda_tidy %>%
group_by(topic) %>%
top_n(8, beta) %>%
ungroup() %>%
arrange(topic, -beta)
head(top_terms, 10)
# visualize with ggplot
# NOTE: We want to treat the topics as categorical,
# so turn it to a factor first.
# NOTE: Don't show legend because it's not helpful.
top_terms %>%
mutate(term = reorder(term, beta)) %>%
ggplot(aes(term,
beta,
fill = factor(topic))) +
geom_col(show.legend=FALSE) +
facet_wrap( ~ topic, scales="free") +
coord_flip()
write.csv(prompt_lda_topics,
file = paste("prompt_LDA_",
k,
".csv",
sep = ""))
prompt_lda_document_topics <- tidy(prompt_lda,
matrix="gamma")
head(prompt_lda_document_topics)
prompt_lda_document_topics <- tidy(prompt_lda,
matrix="gamma")
head(prompt_lda_document_topics)
tail(prompt_lda_document_topics)
View(prompt_lda_document_topics)
write.csv(prompt_lda_document_topics,
file = paste("prompt_topic_gamma_match",
k,
".csv",
sep = ""))
# same as nrow() and ncol()
dim(prompt_lda_document_topics)
prompt_lda_document <- spread(prompt_lda_document_topics,
topic, # the col to spread
gamma) # fill in the new cols with gamma
dim(prompt_lda_document)
head(prompt_lda_document)
k <- 2
# NOTE: Optimal k varies, so you'll need to experiment
# with which k value is best.
prompt_lda <- LDA(prompt_dtm,
k = k,
control = list(seed=42))
prompt_lda
# NOTE: seed keeps randomization consistent
# get the words out of our LDA
prompt_lda_words <- terms(prompt_lda, 5)
prompt_lda_words
# NOTE: at this stage you may want to go back
# and add some stop words to your posts based
# on the results you got from LDA.
# Remove words that don't add any meaning
# to the analysis.
# Save LDA words as CSV --------------
# need a matrix format to be written to csv
prompt_lda_topics <- as.matrix(prompt_lda_words)
# write.csv(prompt_lda_topics,
#           file = paste("prompt_LDA_",
#                        k,
#                        ".csv",
#                        sep = ""))
# NOTE: k variable is helpful here when you run many LDAs
# with different k's
# Examine the topics  --------------
head(prompt_lda_topics)
# turn our LDA into the
# tidy format of the tidyverse packages
prompt_lda_tidy <- tidy(prompt_lda)
# Beta shows how much each term
# contributes to each topic
# NOTE: Typically list 8 terms when topic modelling
top_terms <- prompt_lda_tidy %>%
group_by(topic) %>%
top_n(8, beta) %>%
ungroup() %>%
arrange(topic, -beta)
head(top_terms, 10)
# visualize with ggplot
# NOTE: We want to treat the topics as categorical,
# so turn it to a factor first.
# NOTE: Don't show legend because it's not helpful.
top_terms %>%
mutate(term = reorder(term, beta)) %>%
ggplot(aes(term,
beta,
fill = factor(topic))) +
geom_col(show.legend=FALSE) +
facet_wrap( ~ topic, scales="free") +
coord_flip()
# GAMMA -------------
# How much does each topic contribute to each document?
# --> Use GAMMA statistic
# (As opposed to how much each word contributes
# to each topic, which is beta)
prompt_lda_document_topics <- tidy(prompt_lda,
matrix="gamma")
head(prompt_lda_document_topics)
tail(prompt_lda_document_topics)
View(prompt_lda_document_topics)
prompt_lda_document <- spread(prompt_lda_document_topics,
topic, # the col to spread
gamma) # fill in the new cols with gamma
dim(prompt_lda_document)
head(prompt_lda_document)
# k is the number of topics we want
# (like k means clustering?)
k <- 3
# NOTE: Optimal k varies, so you'll need to experiment
# with which k value is best.
prompt_lda <- LDA(prompt_dtm,
k = k,
control = list(seed=42))
prompt_lda
# NOTE: seed keeps randomization consistent
# get the words out of our LDA
prompt_lda_words <- terms(prompt_lda, 5)
prompt_lda_words
# NOTE: at this stage you may want to go back
# and add some stop words to your posts based
# on the results you got from LDA.
# Remove words that don't add any meaning
# to the analysis.
# Save LDA words as CSV --------------
# need a matrix format to be written to csv
prompt_lda_topics <- as.matrix(prompt_lda_words)
# write.csv(prompt_lda_topics,
#           file = paste("prompt_LDA_",
#                        k,
#                        ".csv",
#                        sep = ""))
# NOTE: k variable is helpful here when you run many LDAs
# with different k's
# Examine the topics  --------------
head(prompt_lda_topics)
# turn our LDA into the
# tidy format of the tidyverse packages
prompt_lda_tidy <- tidy(prompt_lda)
# Beta shows how much each term
# contributes to each topic
# NOTE: Typically list 8 terms when topic modelling
top_terms <- prompt_lda_tidy %>%
group_by(topic) %>%
top_n(8, beta) %>%
ungroup() %>%
arrange(topic, -beta)
head(top_terms, 10)
# visualize with ggplot
# NOTE: We want to treat the topics as categorical,
# so turn it to a factor first.
# NOTE: Don't show legend because it's not helpful.
top_terms %>%
mutate(term = reorder(term, beta)) %>%
ggplot(aes(term,
beta,
fill = factor(topic))) +
geom_col(show.legend=FALSE) +
facet_wrap( ~ topic, scales="free") +
coord_flip()
# GAMMA -------------
# How much does each topic contribute to each document?
# --> Use GAMMA statistic
# (As opposed to how much each word contributes
# to each topic, which is beta)
prompt_lda_document_topics <- tidy(prompt_lda,
matrix="gamma")
head(prompt_lda_document_topics)
tail(prompt_lda_document_topics)
# write.csv(prompt_lda_document_topics,
#           file = paste("prompt_topic_gamma_match",
#                        k,
#                        ".csv",
#                        sep = ""))
# same as nrow() and ncol()
dim(prompt_lda_document_topics)
# Have each topic as a different column --------------
# Ex: topic1, topic2, topic3
# As opposed to one topic column
# NOTE: gamma values are to fill in the new columns
prompt_lda_document <- spread(prompt_lda_document_topics,
topic, # the col to spread
gamma) # fill in the new cols with gamma
dim(prompt_lda_document)
head(prompt_lda_document)
prompt_lda_document$max_topic <-
colnames(prompt_lda_document[2:6])[apply(X=prompt_lda_document,
MARGIN=1,
FUN=which.max)]
prompt_lda_document$max_topic <-
colnames(prompt_lda_document[2:4])[apply(X=prompt_lda_document,
MARGIN=1,
FUN=which.max)]
dt1 <- data.table(prompt_lda_document,
key = "document")
dt2 <- data.table(prompt_df,
key = "doc_id")
prompt_merged <- dt1[dt2]
dim(prompt_merged)
colnames(prompt_merged)
prompt_analyze <- select(prompt_merged,
c(document,
text,
score,
num_comments,
url,
created))
head(prompt_analyze)
# for sentiments
library(topicmodels)
sentiments
# textdata and nrc may need package installations
library("textdata")
get_sentiments("afinn")
get_sentiments("nrc")
get_sentiments("bing")
# convert csv data into a format
# the tidyverse library can work with
tidy_prompts <- prompt_df %>%
ungroup() %>%
unnest_tokens(word, prompt)
# from the conversion above,
# we have a new column called "word"
summary(tidy_prompts)
head(tidy_prompts)
colnames(tidy_prompts)
# Get all the "joy" sentiment words
# from the nrc dictionary
nrc_sent <- get_sentiments("nrc") %>%
filter(sentiment == "joy")
# --> Which words do top writing prompts express joy with
# inner join nrc_sent with prompts
tidy_prompts %>%
inner_join(nrc_sent) %>%
dplyr::count(word, sort = TRUE) # important to specify dplyer's count
# --> Difference of positive and negative words in each prompt
# ex: if one is lots of negative words, we get a negative score
# ex: if one is lots of positive words, we get a positive score
prompts_sentiment <- tidy_prompts %>%
inner_join(get_sentiments("bing")) %>%
dplyr::count(doc_id, sentiment) %>%
spread(sentiment, n, fill = 0) %>%
mutate(sentiment = positive - negative)
head(prompts_sentiment)
# visualize positive words vs negative words in prompt
ggplot(prompts_sentiment, aes(negative, positive)) +
geom_point(show.legend = FALSE,
size = 15,
shape = "square",
alpha = 0.1)
# for sentiments
library(topicmodels)
sentiments
# textdata and nrc may need package installations
library("textdata")
get_sentiments("afinn")
get_sentiments("nrc")
get_sentiments("bing")
# convert csv data into a format
# the tidyverse library can work with
tidy_prompts <- prompt_df %>%
ungroup() %>%
unnest_tokens(word, prompt)
# convert csv data into a format
# the tidyverse library can work with
tidy_prompts <- prompt_df %>%
ungroup() %>%
unnest_tokens(word, prompt)
# convert csv data into a format
# the tidyverse library can work with
colnames(prompt_df)
tidy_prompts <- prompt_df %>%
ungroup() %>%
unnest_tokens(word, text)
# from the conversion above,
# we have a new column called "word"
summary(tidy_prompts)
head(tidy_prompts)
colnames(tidy_prompts)
nrc_sent <- get_sentiments("nrc") %>%
filter(sentiment == "joy")
# --> Which words do top writing prompts express joy with
# inner join nrc_sent with prompts
tidy_prompts %>%
inner_join(nrc_sent) %>%
dplyr::count(word, sort = TRUE) # important to specify dplyer's count
joy_words <- tidy_prompts %>%
inner_join(nrc_sent) %>%
dplyr::count(word, sort = TRUE) # important to specify dplyer's count
joy_words
# --> Difference of positive and negative words in each prompt
# ex: if one is lots of negative words, we get a negative score
# ex: if one is lots of positive words, we get a positive score
prompts_sentiment <- tidy_prompts %>%
inner_join(get_sentiments("bing")) %>%
dplyr::count(doc_id, sentiment) %>%
spread(sentiment, n, fill = 0) %>%
mutate(sentiment = positive - negative)
head(prompts_sentiment)
# visualize positive words vs negative words in prompt
ggplot(prompts_sentiment, aes(negative, positive)) +
geom_point(show.legend = FALSE,
size = 15,
shape = "square",
alpha = 0.1)
all_prompts_sentiment <- mean(prompts_sentiment$sentiment)
all_prompts_sentiment
write.csv(prompts_sentiment,
file = paste("prompts_sentiments",
k,
".csv",
sep = ""))
write.csv(get_sentiments("bing"),
file = paste("bing_sentiments",
k,
".csv",
sep = ""))
###################################################
# NLP - SUBREDDIT WRITING PROMPTS
###################################################
# IMPORT LIBRARIES
#to create and work with corpora
library(tm)
#for LDA topic models
library(topicmodels)
# other useful packages
library(tidyverse)
library(tidytext)
library(stringr)
# dplyr must be placed below data.table
library(data.table)
library(dplyr)
###################################################
# SETUP DIRECTORY
# Clear R's memory.
rm(list=ls())
# Set working directory
setwd("C:/Users/Oscar Ko/Desktop/100-Days-of-Data-Projects/9-NLP_WritingPrompts")
# Check current working directory
getwd()
# Lists files in current working directory
list.files()
###################################################
# IMPORT DATA
# Faster way to import CSV with "vroom" package
library(vroom)
data <- vroom("WritingPrompts_data.csv")
###################################################
# CHECK THE DATA
head(data)
tail(data)
str(data)
summary(data)
# check number of rows and columns
nrow(data)
ncol(data)
# check column names
colnames(data)
# check for missing values
any(is.na(data))
# Amelia package for visualizing missing data
library(Amelia)
missmap(data,
main = "Missing Map",
col = c("red", "black"),
legend = T)
###################################################
# KEEP ONLY RELEVANT COLUMNS
library(tidyverse)
title_df <- select(data, id, title, score, num_comments, url, created)
missmap(title_df,
main = "Missing Map",
col = c("red", "black"),
legend = T)
head(title_df)
###################################################
# DATA CLEANING
# Separate titles into tags and prompts ------------
tags <- c()
prompts <- c()
for (txt in title_df$title) {
split_txt = strsplit(txt, "]")
tag = substring(split_txt[[1]][1], 2)
prompt = split_txt[[1]][2]
tags <- c(tags, tag)
prompts <- c(prompts, prompt)
}
title_df$tag <- tags
title_df$prompt <- prompts
head(title_df)
unique(title_df$tag)
# Fix a row anomoly -----------------------------------------------
error_tag <- " man who sees ghosts checks himself into a mental institution, oblivious to the fact that the facility has been closed for almost thirty years. [WP"
title_df[title_df$tag == error_tag, ]$title
the_title <- "[WP] A man who sees ghosts checks himself into a mental institution, oblivious to the fact that the facility has been closed for almost thirty years."
the_tag <- "WP"
the_prompt <- "A man who sees ghosts checks himself into a mental institution, oblivious to the fact that the facility has been closed for almost thirty years."
title_df <- title_df[title_df$tag != error_tag, ]
title_df <- rbind(title_df, c(the_title, the_tag, the_prompt))
tail(title_df)
# Keep only standard [WP] posts. Filter out everything else. --------
unique(title_df$tag)
title_df <- title_df[title_df$tag %in% c("WP", "Wp", " WP "), ]
unique(title_df$tag)
nrow(title_df)
# Create prompts dataframe ------------------------------------------
prompt_df <- select(title_df, id, prompt, score, num_comments, url, created)
head(prompt_df)
tail(prompt_df)
write.csv(prompt_df,
file = paste("cleaned_prompt_df",
k,
".csv",
sep = ""))
write.csv(prompt_df,
file = paste("cleaned_prompt_df",
".csv",
sep = ""))
