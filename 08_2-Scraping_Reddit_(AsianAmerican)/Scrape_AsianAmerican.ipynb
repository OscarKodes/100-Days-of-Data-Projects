{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd7df194",
   "metadata": {},
   "source": [
    "# SCRAPING AsianAmerican SUBREDDIT\n",
    "\n",
    "Referenced Link: https://towardsdatascience.com/scraping-reddit-data-1c0af3040768"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f57b17",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "# SETUP STEPS:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c9416b",
   "metadata": {},
   "source": [
    "## Import env for API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0c9bde3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6590cb",
   "metadata": {},
   "source": [
    "## Create PRAW instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc10556e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "\n",
    "reddit = praw.Reddit(client_id=os.getenv(\"my_client_id\"), \n",
    "                     client_secret=os.getenv(\"my_client_secret\"), \n",
    "                     user_agent=os.getenv(\"my_user_agent\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732eb53c",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---\n",
    "\n",
    "\n",
    "# CREATE TWO DATA FRAMES\n",
    "\n",
    "- ## Hot Posts dataframe\n",
    "- ## Top Comments from Hot Posts dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72237a9b",
   "metadata": {},
   "source": [
    "### Create top posts dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2550345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "999 posts scraped.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>url</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>body</th>\n",
       "      <th>created</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NBA All-Star Damian Lillard wears \"\"Stop Asian...</td>\n",
       "      <td>1604</td>\n",
       "      <td>me5ef9</td>\n",
       "      <td>asianamerican</td>\n",
       "      <td>https://i.redd.it/rey9pga6lhp61.jpg</td>\n",
       "      <td>54</td>\n",
       "      <td></td>\n",
       "      <td>1.616815e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>‚ÄúFuck you! We will stop the Hate!‚Äù NBA star Ba...</td>\n",
       "      <td>1375</td>\n",
       "      <td>m35vd1</td>\n",
       "      <td>asianamerican</td>\n",
       "      <td>https://i.redd.it/sd9p46xlzhm61.jpg</td>\n",
       "      <td>58</td>\n",
       "      <td></td>\n",
       "      <td>1.615512e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Accurate</td>\n",
       "      <td>1285</td>\n",
       "      <td>gkut94</td>\n",
       "      <td>asianamerican</td>\n",
       "      <td>https://i.redd.it/gbzzxwnir4z41.jpg</td>\n",
       "      <td>44</td>\n",
       "      <td></td>\n",
       "      <td>1.589636e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Naomi Osaka: \"If people loved Asian people as ...</td>\n",
       "      <td>1279</td>\n",
       "      <td>mf1q4w</td>\n",
       "      <td>asianamerican</td>\n",
       "      <td>https://twitter.com/naomiosaka/status/13758652...</td>\n",
       "      <td>87</td>\n",
       "      <td></td>\n",
       "      <td>1.616942e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>My friend's mother was one of the victims of A...</td>\n",
       "      <td>1194</td>\n",
       "      <td>m885fx</td>\n",
       "      <td>asianamerican</td>\n",
       "      <td>https://gofund.me/6653b648</td>\n",
       "      <td>51</td>\n",
       "      <td></td>\n",
       "      <td>1.616124e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  score      id  \\\n",
       "0  NBA All-Star Damian Lillard wears \"\"Stop Asian...   1604  me5ef9   \n",
       "1  ‚ÄúFuck you! We will stop the Hate!‚Äù NBA star Ba...   1375  m35vd1   \n",
       "2                                           Accurate   1285  gkut94   \n",
       "3  Naomi Osaka: \"If people loved Asian people as ...   1279  mf1q4w   \n",
       "4  My friend's mother was one of the victims of A...   1194  m885fx   \n",
       "\n",
       "       subreddit                                                url  \\\n",
       "0  asianamerican                https://i.redd.it/rey9pga6lhp61.jpg   \n",
       "1  asianamerican                https://i.redd.it/sd9p46xlzhm61.jpg   \n",
       "2  asianamerican                https://i.redd.it/gbzzxwnir4z41.jpg   \n",
       "3  asianamerican  https://twitter.com/naomiosaka/status/13758652...   \n",
       "4  asianamerican                         https://gofund.me/6653b648   \n",
       "\n",
       "   num_comments body       created  \n",
       "0            54       1.616815e+09  \n",
       "1            58       1.615512e+09  \n",
       "2            44       1.589636e+09  \n",
       "3            87       1.616942e+09  \n",
       "4            51       1.616124e+09  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# create empty list to gather raw post data\n",
    "recorded_posts = []\n",
    "\n",
    "# create instance of PRAW for subreddit\n",
    "top_posts = reddit.subreddit('AsianAmerican').top(time_filter=\"all\",\n",
    "                                                 limit=None)\n",
    "\n",
    "# loop through PRAW instance and record in post list\n",
    "for post in top_posts:\n",
    "    \n",
    "    recorded_posts.append([post.title, \n",
    "                  post.score, \n",
    "                  post.id, \n",
    "                  post.subreddit, \n",
    "                  post.url, \n",
    "                  post.num_comments, \n",
    "                  post.selftext, \n",
    "                  post.created])\n",
    "\n",
    "# create dataframe for posts\n",
    "top_posts_df = pd.DataFrame(recorded_posts,\n",
    "                     columns=['title', \n",
    "                              'score', \n",
    "                              'id', \n",
    "                              'subreddit', \n",
    "                              'url', \n",
    "                              'num_comments', \n",
    "                              'body', \n",
    "                              'created'])\n",
    "\n",
    "print(top_posts_df.shape[0], \"posts scraped.\")\n",
    "top_posts_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d197609",
   "metadata": {},
   "source": [
    "### Create comments dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bd7f1d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping post with index number: 0\n",
      "Scraping post with index number: 10\n",
      "Scraping post with index number: 20\n",
      "Scraping post with index number: 30\n",
      "Scraping post with index number: 40\n",
      "Scraping post with index number: 50\n",
      "Scraping post with index number: 60\n",
      "Scraping post with index number: 70\n",
      "Scraping post with index number: 80\n",
      "Scraping post with index number: 90\n",
      "Scraping post with index number: 100\n",
      "Scraping post with index number: 110\n",
      "Scraping post with index number: 120\n",
      "Scraping post with index number: 130\n",
      "Scraping post with index number: 140\n",
      "Scraping post with index number: 150\n",
      "Scraping post with index number: 160\n",
      "Scraping post with index number: 170\n",
      "Scraping post with index number: 180\n",
      "Scraping post with index number: 190\n",
      "Scraping post with index number: 200\n",
      "Scraping post with index number: 210\n",
      "Scraping post with index number: 220\n",
      "Scraping post with index number: 230\n",
      "Scraping post with index number: 240\n",
      "Scraping post with index number: 250\n",
      "Scraping post with index number: 260\n",
      "Scraping post with index number: 270\n",
      "Scraping post with index number: 280\n",
      "Scraping post with index number: 290\n",
      "Scraping post with index number: 300\n",
      "Scraping post with index number: 310\n",
      "Scraping post with index number: 320\n",
      "Scraping post with index number: 330\n",
      "Scraping post with index number: 340\n",
      "Scraping post with index number: 350\n",
      "Scraping post with index number: 360\n",
      "Scraping post with index number: 370\n",
      "Scraping post with index number: 380\n",
      "Scraping post with index number: 390\n",
      "Scraping post with index number: 400\n",
      "Scraping post with index number: 410\n",
      "Scraping post with index number: 420\n",
      "Scraping post with index number: 430\n",
      "Scraping post with index number: 440\n",
      "Scraping post with index number: 450\n",
      "Scraping post with index number: 460\n",
      "Scraping post with index number: 470\n",
      "Scraping post with index number: 480\n",
      "Scraping post with index number: 490\n",
      "Scraping post with index number: 500\n",
      "Scraping post with index number: 510\n",
      "Scraping post with index number: 520\n",
      "Scraping post with index number: 530\n",
      "Scraping post with index number: 540\n",
      "Scraping post with index number: 550\n",
      "Scraping post with index number: 560\n",
      "Scraping post with index number: 570\n",
      "Scraping post with index number: 580\n",
      "Scraping post with index number: 590\n",
      "Scraping post with index number: 600\n",
      "Scraping post with index number: 610\n",
      "Scraping post with index number: 620\n",
      "Scraping post with index number: 630\n",
      "Scraping post with index number: 640\n",
      "Scraping post with index number: 650\n",
      "Scraping post with index number: 660\n",
      "Scraping post with index number: 670\n",
      "Scraping post with index number: 680\n",
      "Scraping post with index number: 690\n",
      "Scraping post with index number: 700\n",
      "Scraping post with index number: 710\n",
      "Scraping post with index number: 720\n",
      "Scraping post with index number: 730\n",
      "Scraping post with index number: 740\n",
      "Scraping post with index number: 750\n",
      "Scraping post with index number: 760\n",
      "Scraping post with index number: 770\n",
      "Scraping post with index number: 780\n",
      "Scraping post with index number: 790\n",
      "Scraping post with index number: 800\n",
      "Scraping post with index number: 810\n",
      "Scraping post with index number: 820\n",
      "Scraping post with index number: 830\n",
      "Scraping post with index number: 840\n",
      "Scraping post with index number: 850\n",
      "Scraping post with index number: 860\n",
      "Scraping post with index number: 870\n",
      "Scraping post with index number: 880\n",
      "Scraping post with index number: 890\n",
      "Scraping post with index number: 900\n",
      "Scraping post with index number: 910\n",
      "Scraping post with index number: 920\n",
      "Scraping post with index number: 930\n",
      "Scraping post with index number: 940\n",
      "Scraping post with index number: 950\n",
      "Scraping post with index number: 960\n",
      "Scraping post with index number: 970\n",
      "Scraping post with index number: 980\n",
      "Scraping post with index number: 990\n",
      "13819 top comments scraped.\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 13819 entries, 0 to 12\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   post_id  13819 non-null  object\n",
      " 1   body     13819 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 323.9+ KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>me5ef9</td>\n",
       "      <td>‚úä‚úäüèø‚úä‚úäüèø‚úä‚úäüèø</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>me5ef9</td>\n",
       "      <td>The high school he went to in Oakland is 50% A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>me5ef9</td>\n",
       "      <td>‚úä‚úä‚úä‚úä‚úä‚úä</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>me5ef9</td>\n",
       "      <td>[deleted]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>me5ef9</td>\n",
       "      <td>Love to see the support. Anyone know where to ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  post_id                                               body\n",
       "0  me5ef9                                          ‚úä‚úäüèø‚úä‚úäüèø‚úä‚úäüèø\n",
       "1  me5ef9  The high school he went to in Oakland is 50% A...\n",
       "2  me5ef9                                             ‚úä‚úä‚úä‚úä‚úä‚úä\n",
       "3  me5ef9                                          [deleted]\n",
       "4  me5ef9  Love to see the support. Anyone know where to ..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from praw.models import MoreComments\n",
    "\n",
    "# create dataframe for comments\n",
    "comments_df = pd.DataFrame(columns=['post_id', 'body'])\n",
    "\n",
    "\n",
    "# loop through ids in posts, and gather all the top comments into dataframe\n",
    "for i, post_id in enumerate(top_posts_df.id):\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        \n",
    "        print(\"Scraping post with index number:\", i)\n",
    "    \n",
    "    comments = []\n",
    "\n",
    "    submission = reddit.submission(id=post_id)\n",
    "\n",
    "    for comment in submission.comments:\n",
    "\n",
    "        if isinstance(comment, MoreComments):\n",
    "\n",
    "            continue\n",
    "\n",
    "        comments.append([post_id, comment.body])\n",
    "\n",
    "    comments = pd.DataFrame(comments,\n",
    "                         columns=['post_id', 'body'])\n",
    "    \n",
    "    comments_df = pd.concat([comments_df, comments], sort=False)\n",
    "\n",
    "    \n",
    "print(comments_df.shape[0], \"top comments scraped.\\n\")\n",
    "\n",
    "print(comments_df.info())\n",
    "\n",
    "comments_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845b0653",
   "metadata": {},
   "source": [
    "# EXPORT to CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3bc93c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_posts_df.to_csv(\"AsianAmerican_posts.csv\", sep=',')\n",
    "\n",
    "comments_df.to_csv(\"AsianAmerican_comments.csv\", sep=',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
