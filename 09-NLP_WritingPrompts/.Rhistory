prompt_lda_words <- terms(prompt_lda, 5)
prompt_lda_words
# Save LDA words as CSV --------------
# need a matrix format to be written to csv
prompt_lda_topics <- as.matrix(prompt_lda_words)
# write.csv(prompt_lda_topics,
#           file = paste("prompt_LDA_",
#                        k,
#                        ".csv",
#                        sep = ""))
# NOTE: k variable is helpful here when you run many LDAs
# with different k's
# Examine the topics  --------------
head(prompt_lda_topics)
# turn our LDA into the
# tidy format of the tidyverse packages
prompt_lda_tidy <- tidy(prompt_lda)
# Beta shows how much each term
# contributes to each topic
# NOTE: Typically list 8 terms when topic modelling
top_terms <- prompt_lda_tidy %>%
group_by(topic) %>%
top_n(8, beta) %>%
ungroup() %>%
arrange(topic, -beta)
head(top_terms, 10)
# NOTE: We want to treat the topics as categorical,
# so turn it to a factor first.
# NOTE: Don't show legend because it's not helpful.
top_terms %>%
mutate(term = reorder(term, beta)) %>%
ggplot(aes(term,
beta,
fill = factor(topic))) +
geom_col(show.legend=FALSE) +
facet_wrap( ~ topic, scales="free") +
coord_flip()
# Remove stop words
my_stops <- c(stopwords("en"),
"just", "can", "like",
"one", "youre", "get",
"find", "day")
prompt_cleaned <- tm_map(prompt_cleaned, removeWords, my_stops)
# Remove white space
prompt_cleaned <- tm_map(prompt_cleaned, stripWhitespace)
# Checking the 3rd document's uncleaned text
prompt_corpus[[3]][1]
# Checking the 3rd document's cleaned text
prompt_cleaned[[3]][1]
#########################################################################
# TOPIC MODELING (DOCUMENT TERM MATRIX)
prompt_dtm <- DocumentTermMatrix(prompt_cleaned)
prompt_dtm
# Sparse entries are 0's, so a specific term does
# not occur in that document.
# There are about 5,853 unique words across our documents.
# They don't appear in every document.
# a little more cleaning to remove empty rows
# keep only indexes of unique words
unique_indexes <- unique(prompt_dtm$i)
prompt_dtm <- prompt_dtm[unique_indexes,]
prompt_dtm
# turn our unique words into the
# tidy format of the tidyverse packages
prompt_dtm_tidy <- tidy(prompt_dtm)
prompt_dtm_tidy
# LDA ANALYSIS ------------------------------------
# k is the number of topics we want
# (like k means clustering?)
k <- 3
# NOTE: Optimal k varies, so you'll need to experiment
# with which k value is best.
prompt_lda <- LDA(prompt_dtm,
k = k,
control = list(seed=42))
prompt_lda
# NOTE: seed keeps randomization consistent
# get the words out of our LDA
prompt_lda_words <- terms(prompt_lda, 5)
prompt_lda_words
# NOTE: at this stage you may want to go back
# and add some stop words to your posts based
# on the results you got from LDA.
# Remove words that don't add any meaning
# to the analysis.
# Save LDA words as CSV --------------
# need a matrix format to be written to csv
prompt_lda_topics <- as.matrix(prompt_lda_words)
# write.csv(prompt_lda_topics,
#           file = paste("prompt_LDA_",
#                        k,
#                        ".csv",
#                        sep = ""))
# NOTE: k variable is helpful here when you run many LDAs
# with different k's
# Examine the topics  --------------
head(prompt_lda_topics)
# turn our LDA into the
# tidy format of the tidyverse packages
prompt_lda_tidy <- tidy(prompt_lda)
# Beta shows how much each term
# contributes to each topic
# NOTE: Typically list 8 terms when topic modelling
top_terms <- prompt_lda_tidy %>%
group_by(topic) %>%
top_n(8, beta) %>%
ungroup() %>%
arrange(topic, -beta)
head(top_terms, 10)
# # Export to CSV
#
# write.csv(top_terms,
#           file = paste("01-prompt_LDA_",
#                        k,
#                        ".csv",
#                        sep = ""))
# visualize with ggplot
# NOTE: We want to treat the topics as categorical,
# so turn it to a factor first.
# NOTE: Don't show legend because it's not helpful.
top_terms %>%
mutate(term = reorder(term, beta)) %>%
ggplot(aes(term,
beta,
fill = factor(topic))) +
geom_col(show.legend=FALSE) +
facet_wrap( ~ topic, scales="free") +
coord_flip()
# Remove stop words
my_stops <- c(stopwords("en"),
"just", "can", "like",
"one", "youre", "get",
"find", "day", "human",
"humans", "world")
prompt_cleaned <- tm_map(prompt_cleaned, removeWords, my_stops)
# Remove white space
prompt_cleaned <- tm_map(prompt_cleaned, stripWhitespace)
# Checking the 3rd document's uncleaned text
prompt_corpus[[3]][1]
# Checking the 3rd document's cleaned text
prompt_cleaned[[3]][1]
#########################################################################
# TOPIC MODELING (DOCUMENT TERM MATRIX)
prompt_dtm <- DocumentTermMatrix(prompt_cleaned)
prompt_dtm
# Sparse entries are 0's, so a specific term does
# not occur in that document.
# There are about 5,853 unique words across our documents.
# They don't appear in every document.
# a little more cleaning to remove empty rows
# keep only indexes of unique words
unique_indexes <- unique(prompt_dtm$i)
prompt_dtm <- prompt_dtm[unique_indexes,]
prompt_dtm
# turn our unique words into the
# tidy format of the tidyverse packages
prompt_dtm_tidy <- tidy(prompt_dtm)
prompt_dtm_tidy
# LDA ANALYSIS ------------------------------------
# k is the number of topics we want
# (like k means clustering?)
k <- 3
# NOTE: Optimal k varies, so you'll need to experiment
# with which k value is best.
prompt_lda <- LDA(prompt_dtm,
k = k,
control = list(seed=42))
prompt_lda
# NOTE: seed keeps randomization consistent
# get the words out of our LDA
prompt_lda_words <- terms(prompt_lda, 5)
prompt_lda_words
# NOTE: at this stage you may want to go back
# and add some stop words to your posts based
# on the results you got from LDA.
# Remove words that don't add any meaning
# to the analysis.
# Save LDA words as CSV --------------
# need a matrix format to be written to csv
prompt_lda_topics <- as.matrix(prompt_lda_words)
# write.csv(prompt_lda_topics,
#           file = paste("prompt_LDA_",
#                        k,
#                        ".csv",
#                        sep = ""))
# NOTE: k variable is helpful here when you run many LDAs
# with different k's
# Examine the topics  --------------
head(prompt_lda_topics)
# turn our LDA into the
# tidy format of the tidyverse packages
prompt_lda_tidy <- tidy(prompt_lda)
# Beta shows how much each term
# contributes to each topic
# NOTE: Typically list 8 terms when topic modelling
top_terms <- prompt_lda_tidy %>%
group_by(topic) %>%
top_n(8, beta) %>%
ungroup() %>%
arrange(topic, -beta)
head(top_terms, 10)
# # Export to CSV
#
# write.csv(top_terms,
#           file = paste("01-prompt_LDA_",
#                        k,
#                        ".csv",
#                        sep = ""))
# visualize with ggplot
# NOTE: We want to treat the topics as categorical,
# so turn it to a factor first.
# NOTE: Don't show legend because it's not helpful.
top_terms %>%
mutate(term = reorder(term, beta)) %>%
ggplot(aes(term,
beta,
fill = factor(topic))) +
geom_col(show.legend=FALSE) +
facet_wrap( ~ topic, scales="free") +
coord_flip()
# Remove stop words
my_stops <- c(stopwords("en"),
"just", "can", "like",
"one", "youre", "get",
"find", "day", "human",
"humans", "world", "time",
"life")
prompt_cleaned <- tm_map(prompt_cleaned, removeWords, my_stops)
# Remove white space
prompt_cleaned <- tm_map(prompt_cleaned, stripWhitespace)
# Checking the 3rd document's uncleaned text
prompt_corpus[[3]][1]
# Checking the 3rd document's cleaned text
prompt_cleaned[[3]][1]
#########################################################################
# TOPIC MODELING (DOCUMENT TERM MATRIX)
prompt_dtm <- DocumentTermMatrix(prompt_cleaned)
prompt_dtm
# Sparse entries are 0's, so a specific term does
# not occur in that document.
# There are about 5,853 unique words across our documents.
# They don't appear in every document.
# a little more cleaning to remove empty rows
# keep only indexes of unique words
unique_indexes <- unique(prompt_dtm$i)
prompt_dtm <- prompt_dtm[unique_indexes,]
prompt_dtm
# turn our unique words into the
# tidy format of the tidyverse packages
prompt_dtm_tidy <- tidy(prompt_dtm)
prompt_dtm_tidy
# LDA ANALYSIS ------------------------------------
# k is the number of topics we want
# (like k means clustering?)
k <- 3
# NOTE: Optimal k varies, so you'll need to experiment
# with which k value is best.
prompt_lda <- LDA(prompt_dtm,
k = k,
control = list(seed=42))
prompt_lda
# NOTE: seed keeps randomization consistent
# get the words out of our LDA
prompt_lda_words <- terms(prompt_lda, 5)
prompt_lda_words
# NOTE: at this stage you may want to go back
# and add some stop words to your posts based
# on the results you got from LDA.
# Remove words that don't add any meaning
# to the analysis.
# Save LDA words as CSV --------------
# need a matrix format to be written to csv
prompt_lda_topics <- as.matrix(prompt_lda_words)
# write.csv(prompt_lda_topics,
#           file = paste("prompt_LDA_",
#                        k,
#                        ".csv",
#                        sep = ""))
# NOTE: k variable is helpful here when you run many LDAs
# with different k's
# Examine the topics  --------------
head(prompt_lda_topics)
# turn our LDA into the
# tidy format of the tidyverse packages
prompt_lda_tidy <- tidy(prompt_lda)
# Beta shows how much each term
# contributes to each topic
# NOTE: Typically list 8 terms when topic modelling
top_terms <- prompt_lda_tidy %>%
group_by(topic) %>%
top_n(8, beta) %>%
ungroup() %>%
arrange(topic, -beta)
head(top_terms, 10)
# # Export to CSV
#
# write.csv(top_terms,
#           file = paste("01-prompt_LDA_",
#                        k,
#                        ".csv",
#                        sep = ""))
# visualize with ggplot
# NOTE: We want to treat the topics as categorical,
# so turn it to a factor first.
# NOTE: Don't show legend because it's not helpful.
top_terms %>%
mutate(term = reorder(term, beta)) %>%
ggplot(aes(term,
beta,
fill = factor(topic))) +
geom_col(show.legend=FALSE) +
facet_wrap( ~ topic, scales="free") +
coord_flip()
# Remove stop words
my_stops <- c(stopwords("en"),
"just", "can", "like",
"one", "youre", "get",
"find", "day", "human",
"humans", "world", "time",
"life", "years")
prompt_cleaned <- tm_map(prompt_cleaned, removeWords, my_stops)
# Remove white space
prompt_cleaned <- tm_map(prompt_cleaned, stripWhitespace)
# Checking the 3rd document's uncleaned text
prompt_corpus[[3]][1]
# Checking the 3rd document's cleaned text
prompt_cleaned[[3]][1]
#########################################################################
# TOPIC MODELING (DOCUMENT TERM MATRIX)
prompt_dtm <- DocumentTermMatrix(prompt_cleaned)
prompt_dtm
# Sparse entries are 0's, so a specific term does
# not occur in that document.
# There are about 5,853 unique words across our documents.
# They don't appear in every document.
# a little more cleaning to remove empty rows
# keep only indexes of unique words
unique_indexes <- unique(prompt_dtm$i)
prompt_dtm <- prompt_dtm[unique_indexes,]
prompt_dtm
# turn our unique words into the
# tidy format of the tidyverse packages
prompt_dtm_tidy <- tidy(prompt_dtm)
prompt_dtm_tidy
# LDA ANALYSIS ------------------------------------
# k is the number of topics we want
# (like k means clustering?)
k <- 3
# NOTE: Optimal k varies, so you'll need to experiment
# with which k value is best.
prompt_lda <- LDA(prompt_dtm,
k = k,
control = list(seed=42))
prompt_lda
# NOTE: seed keeps randomization consistent
# get the words out of our LDA
prompt_lda_words <- terms(prompt_lda, 5)
prompt_lda_words
# NOTE: at this stage you may want to go back
# and add some stop words to your posts based
# on the results you got from LDA.
# Remove words that don't add any meaning
# to the analysis.
# Save LDA words as CSV --------------
# need a matrix format to be written to csv
prompt_lda_topics <- as.matrix(prompt_lda_words)
# write.csv(prompt_lda_topics,
#           file = paste("prompt_LDA_",
#                        k,
#                        ".csv",
#                        sep = ""))
# NOTE: k variable is helpful here when you run many LDAs
# with different k's
# Examine the topics  --------------
head(prompt_lda_topics)
# turn our LDA into the
# tidy format of the tidyverse packages
prompt_lda_tidy <- tidy(prompt_lda)
# Beta shows how much each term
# contributes to each topic
# NOTE: Typically list 8 terms when topic modelling
top_terms <- prompt_lda_tidy %>%
group_by(topic) %>%
top_n(8, beta) %>%
ungroup() %>%
arrange(topic, -beta)
head(top_terms, 10)
top_terms %>%
mutate(term = reorder(term, beta)) %>%
ggplot(aes(term,
beta,
fill = factor(topic))) +
geom_col(show.legend=FALSE) +
facet_wrap( ~ topic, scales="free") +
coord_flip()
# Remove stop words
my_stops <- c(stopwords("en"),
"just", "can", "like",
"one", "youre", "get",
"find", "day", "human",
"humans", "world", "time",
"life", "years", "now")
prompt_cleaned <- tm_map(prompt_cleaned, removeWords, my_stops)
# Remove white space
prompt_cleaned <- tm_map(prompt_cleaned, stripWhitespace)
# Checking the 3rd document's uncleaned text
prompt_corpus[[3]][1]
# Checking the 3rd document's cleaned text
prompt_cleaned[[3]][1]
#########################################################################
# TOPIC MODELING (DOCUMENT TERM MATRIX)
prompt_dtm <- DocumentTermMatrix(prompt_cleaned)
prompt_dtm
# Sparse entries are 0's, so a specific term does
# not occur in that document.
# There are about 5,853 unique words across our documents.
# They don't appear in every document.
# a little more cleaning to remove empty rows
# keep only indexes of unique words
unique_indexes <- unique(prompt_dtm$i)
prompt_dtm <- prompt_dtm[unique_indexes,]
prompt_dtm
# turn our unique words into the
# tidy format of the tidyverse packages
prompt_dtm_tidy <- tidy(prompt_dtm)
prompt_dtm_tidy
# LDA ANALYSIS ------------------------------------
# k is the number of topics we want
# (like k means clustering?)
k <- 3
# NOTE: Optimal k varies, so you'll need to experiment
# with which k value is best.
prompt_lda <- LDA(prompt_dtm,
k = k,
control = list(seed=42))
prompt_lda
# NOTE: seed keeps randomization consistent
# get the words out of our LDA
prompt_lda_words <- terms(prompt_lda, 5)
prompt_lda_words
# NOTE: at this stage you may want to go back
# and add some stop words to your posts based
# on the results you got from LDA.
# Remove words that don't add any meaning
# to the analysis.
# Save LDA words as CSV --------------
# need a matrix format to be written to csv
prompt_lda_topics <- as.matrix(prompt_lda_words)
# write.csv(prompt_lda_topics,
#           file = paste("prompt_LDA_",
#                        k,
#                        ".csv",
#                        sep = ""))
# NOTE: k variable is helpful here when you run many LDAs
# with different k's
# Examine the topics  --------------
head(prompt_lda_topics)
# turn our LDA into the
# tidy format of the tidyverse packages
prompt_lda_tidy <- tidy(prompt_lda)
# Beta shows how much each term
# contributes to each topic
# NOTE: Typically list 8 terms when topic modelling
top_terms <- prompt_lda_tidy %>%
group_by(topic) %>%
top_n(8, beta) %>%
ungroup() %>%
arrange(topic, -beta)
head(top_terms, 10)
# # Export to CSV
#
# write.csv(top_terms,
#           file = paste("01-prompt_LDA_",
#                        k,
#                        ".csv",
#                        sep = ""))
# visualize with ggplot
# NOTE: We want to treat the topics as categorical,
# so turn it to a factor first.
# NOTE: Don't show legend because it's not helpful.
top_terms %>%
mutate(term = reorder(term, beta)) %>%
ggplot(aes(term,
beta,
fill = factor(topic))) +
geom_col(show.legend=FALSE) +
facet_wrap( ~ topic, scales="free") +
coord_flip()
write.csv(prompt_lda_topics,
file = paste("(new)prompt_LDA_",
k,
".csv",
sep = ""))
write.csv(top_terms,
file = paste("(new)01-prompt_LDA_",
k,
".csv",
sep = ""))
# GAMMA -------------
# How much does each topic contribute to each document?
# --> Use GAMMA statistic
# (As opposed to how much each word contributes
# to each topic, which is beta)
prompt_lda_document_topics <- tidy(prompt_lda,
matrix="gamma")
head(prompt_lda_document_topics)
tail(prompt_lda_document_topics)
write.csv(prompt_lda_document_topics,
file = paste("(new)prompt_topic_gamma_match",
k,
".csv",
sep = ""))
