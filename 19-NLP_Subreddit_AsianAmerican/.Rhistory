###################################################
# IMPORT LIBRARIES
#to create and work with corpora
library(tm)
#for LDA topic models
library(topicmodels)
# other useful packages
library(tidyverse)
library(tidytext)
library(stringr)
# dplyr must be placed below data.table
library(data.table)
library(dplyr)
###################################################
# SETUP DIRECTORY
# Clear R's memory.
rm(list=ls())
# Set working directory
setwd("C:/Users/Oscar Ko/Desktop/100-Days-of-Data-Projects/19-NLP_Subreddit_AsianAmerican")
# Check current working directory
getwd()
# Lists files in current working directory
list.files()
###################################################
# IMPORT DATA
# Faster way to import CSV with "vroom" package
library(vroom)
comments <- vroom("data/AsianAmerican_comments.csv")
posts <- vroom("data/AsianAmerican_posts.csv")
###################################################
# Combine comment text with original post
colnames(comments)
colnames(posts)
# Join dfs on post_id and id
# keep comment body
# Keep post title (& text if exists)
df <- data.frame(matrix(ncol = 2, nrow = 0))
column_names <- c("post_id", "post_and_comments")
colnames(df) <- columns
for (id in posts$id) {
post_title = c(posts[posts$id == id,]$title)
all_comments = comments[comments$post_id == id,]$body
all_text_vector = c(post_title, all_comments)
text_string = paste(all_text_vector, collapse = ' ')
row = data.frame(
post_id = id,
post_and_comments = text_string
)
df = rbind(df, row)
}
# check number of rows and columns
nrow(data)
# check number of rows and columns
nrow(df)
ncol(df)
# check column names
colnames(df)
# check for missing values
any(is.na(df))
str(df)
summary(df)
head(df)
###################################################
# NLP - SUBREDDIT ASIAN AMERICAN
###################################################
# IMPORT LIBRARIES
#to create and work with corpora
library(tm)
#for LDA topic models
library(topicmodels)
# other useful packages
library(tidyverse)
library(tidytext)
library(stringr)
# dplyr must be placed below data.table
library(data.table)
library(dplyr)
###################################################
# SETUP DIRECTORY
# Clear R's memory.
rm(list=ls())
# Set working directory
setwd("C:/Users/Oscar Ko/Desktop/100-Days-of-Data-Projects/19-NLP_Subreddit_AsianAmerican")
# Check current working directory
getwd()
# Lists files in current working directory
list.files()
###################################################
# IMPORT DATA
# Faster way to import CSV with "vroom" package
library(vroom)
comments <- vroom("data/AsianAmerican_comments.csv")
posts <- vroom("data/AsianAmerican_posts.csv")
###################################################
# Combine comment text with original post
colnames(comments)
colnames(posts)
# Join dfs on post_id and id
# keep comment body
# Keep post title (& text if exists)
df <- data.frame(matrix(ncol = 2, nrow = 0))
column_names <- c("post_id", "post_and_comments")
colnames(df) <- columns
for (id in posts$id) {
post_title = c(posts[posts$id == id,]$title)
all_comments = comments[comments$post_id == id,]$body
all_text_vector = c(post_title, all_comments)
text_string = paste(all_text_vector, collapse = ' ')
row = data.frame(
post_id = id,
post_and_comments = text_string
)
df = rbind(df, row)
}
###################################################
# CHECK THE DATA
head(df)
str(df)
summary(df)
# check number of rows and columns
nrow(df)
ncol(df)
# check column names
colnames(df)
# check for missing values
any(is.na(df))
##############################################################
##############################################################
# NLP SECTIONS
##############################################################
# CREATE CORPUS
# create document id -------------------
# note: for some reason this is quotes sensitive,
# use double quotes
names(df)[names(df)=="post_id"] <- "doc_id"
df$id <- as.character(df$id)
names(df)[names(df)=="post_and_comments"] <- "text"
colnames(df)
# create corpus ------------------------------
# calling DataframeSource to save metadata,
# if you don't want to save metadata, just call VSource()
source <- DataframeSource(df)
corpus <- VCorpus(source)
corpus
# Looking at document number 10 (it can be any number)
corpus[[10]]
corpus[[10]][1] # the text of document 10
corpus[[10]][2] # the metadata of document 10
# clean the corpus ----------------------
# Remove punctuation
cleaned <- tm_map(corpus, removePunctuation)
# Remove numbers
cleaned <- tm_map(cleaned, removeNumbers)
# Make lowercase
cleaned <- tm_map(cleaned, content_transformer(tolower))
# Remove stop words
my_stops <- c(stopwords("en"))
cleaned <- tm_map(cleaned, removeWords, my_stops)
# Remove white space
cleaned <- tm_map(cleaned, stripWhitespace)
# Checking the 3rd document's uncleaned text
corpus[[3]][1]
# Checking the 3rd document's cleaned text
cleaned[[3]][1]
#########################################################################
# TOPIC MODELING (DOCUMENT TERM MATRIX)
dtm <- DocumentTermMatrix(cleaned)
dtm
# Sparse entries are 0's, so a specific term does
# not occur in that document.
# There are about 5,853 unique words across our documents.
# They don't appear in every document.
# a little more cleaning to remove empty rows
# keep only indexes of unique words
unique_indexes <- unique(dtm$i)
dtm <- dtm[unique_indexes,]
dtm
# turn our unique words into the
# tidy format of the tidyverse packages
dtm_tidy <- tidy(dtm)
dtm_tidy
# LDA ANALYSIS ------------------------------------
# k is the number of topics we want
# (like k means clustering?)
k <- 3
lda <- LDA(dtm,
k = k,
control = list(seed=42))
lda
# get the words out of our LDA
lda_words <- terms(lda, 5)
lda_words
# Remove stop words
my_stops <- c(stopwords("en"),
"asian",
"people",
"like",
"just",
"removed")
cleaned <- tm_map(cleaned, removeWords, my_stops)
# Remove white space
cleaned <- tm_map(cleaned, stripWhitespace)
# Checking the 3rd document's uncleaned text
corpus[[3]][1]
# Checking the 3rd document's cleaned text
cleaned[[3]][1]
#########################################################################
# TOPIC MODELING (DOCUMENT TERM MATRIX)
dtm <- DocumentTermMatrix(cleaned)
dtm
# Sparse entries are 0's, so a specific term does
# not occur in that document.
# There are about 5,853 unique words across our documents.
# They don't appear in every document.
# a little more cleaning to remove empty rows
# keep only indexes of unique words
unique_indexes <- unique(dtm$i)
dtm <- dtm[unique_indexes,]
dtm
# turn our unique words into the
# tidy format of the tidyverse packages
dtm_tidy <- tidy(dtm)
dtm_tidy
# LDA ANALYSIS ------------------------------------
# k is the number of topics we want
# (like k means clustering?)
k <- 3
# NOTE: Optimal k varies, so you'll need to experiment
# with which k value is best.
lda <- LDA(dtm,
k = k,
control = list(seed=42))
lda
# NOTE: seed keeps randomization consistent
# get the words out of our LDA
lda_words <- terms(lda, 5)
lda_words
# Remove stop words
my_stops <- c(stopwords("en"),
"asian",
"asians",
"people",
"like",
"just",
"removed",
"dont",
"think",
"even",
"can",
"one",
"get",
"see")
cleaned <- tm_map(cleaned, removeWords, my_stops)
# Remove white space
cleaned <- tm_map(cleaned, stripWhitespace)
# Checking the 3rd document's uncleaned text
corpus[[3]][1]
# Checking the 3rd document's cleaned text
cleaned[[3]][1]
#########################################################################
# TOPIC MODELING (DOCUMENT TERM MATRIX)
dtm <- DocumentTermMatrix(cleaned)
dtm
# Sparse entries are 0's, so a specific term does
# not occur in that document.
# There are about 5,853 unique words across our documents.
# They don't appear in every document.
# a little more cleaning to remove empty rows
# keep only indexes of unique words
unique_indexes <- unique(dtm$i)
dtm <- dtm[unique_indexes,]
dtm
# turn our unique words into the
# tidy format of the tidyverse packages
dtm_tidy <- tidy(dtm)
dtm_tidy
# LDA ANALYSIS ------------------------------------
# k is the number of topics we want
# (like k means clustering?)
k <- 3
# NOTE: Optimal k varies, so you'll need to experiment
# with which k value is best.
lda <- LDA(dtm,
k = k,
control = list(seed=42))
lda
# NOTE: seed keeps randomization consistent
# get the words out of our LDA
lda_words <- terms(lda, 5)
lda_words
# Remove stop words
my_stops <- c(stopwords("en"),
"asian",
"asians",
"people",
"like",
"just",
"removed",
"dont",
"think",
"even",
"can",
"one",
"get",
"see",
"know",
"also",
"really",
"will")
cleaned <- tm_map(cleaned, removeWords, my_stops)
# Remove white space
cleaned <- tm_map(cleaned, stripWhitespace)
# Checking the 3rd document's uncleaned text
corpus[[3]][1]
# Checking the 3rd document's cleaned text
cleaned[[3]][1]
#########################################################################
# TOPIC MODELING (DOCUMENT TERM MATRIX)
dtm <- DocumentTermMatrix(cleaned)
dtm
# Sparse entries are 0's, so a specific term does
# not occur in that document.
# There are about 5,853 unique words across our documents.
# They don't appear in every document.
# a little more cleaning to remove empty rows
# keep only indexes of unique words
unique_indexes <- unique(dtm$i)
dtm <- dtm[unique_indexes,]
dtm
# turn our unique words into the
# tidy format of the tidyverse packages
dtm_tidy <- tidy(dtm)
dtm_tidy
# LDA ANALYSIS ------------------------------------
# k is the number of topics we want
# (like k means clustering?)
k <- 3
# NOTE: Optimal k varies, so you'll need to experiment
# with which k value is best.
lda <- LDA(dtm,
k = k,
control = list(seed=42))
lda
# NOTE: seed keeps randomization consistent
# get the words out of our LDA
lda_words <- terms(lda, 5)
lda_words
# Save LDA words as CSV --------------
# need a matrix format to be written to csv
lda_topics <- as.matrix(lda_words)
# write.csv(lda_topics,
#           file = paste("(new)LDA_",
#                        k,
#                        ".csv",
#                        sep = ""))
# NOTE: k variable is helpful here when you run many LDAs
# with different k's
# Examine the topics  --------------
head(lda_topics)
# turn our LDA into the
# tidy format of the tidyverse packages
lda_tidy <- tidy(lda)
# Beta shows how much each term
# contributes to each topic
# NOTE: Typically list 8 terms when topic modelling
top_terms <- lda_tidy %>%
group_by(topic) %>%
top_n(8, beta) %>%
ungroup() %>%
arrange(topic, -beta)
head(top_terms, 10)
# Export to CSV
#
# write.csv(top_terms,
#           file = paste("(new)01-LDA_",
#                        k,
#                        ".csv",
#                        sep = ""))
#
# visualize with ggplot
# NOTE: We want to treat the topics as categorical,
# so turn it to a factor first.
# NOTE: Don't show legend because it's not helpful.
top_terms %>%
mutate(term = reorder(term, beta)) %>%
ggplot(aes(term,
beta,
fill = factor(topic))) +
geom_col(show.legend=FALSE) +
facet_wrap( ~ topic, scales="free") +
coord_flip()
k <- 2
# NOTE: Optimal k varies, so you'll need to experiment
# with which k value is best.
lda <- LDA(dtm,
k = k,
control = list(seed=42))
lda
# NOTE: seed keeps randomization consistent
# get the words out of our LDA
lda_words <- terms(lda, 5)
lda_words
# need a matrix format to be written to csv
lda_topics <- as.matrix(lda_words)
# write.csv(lda_topics,
#           file = paste("(new)LDA_",
#                        k,
#                        ".csv",
#                        sep = ""))
# NOTE: k variable is helpful here when you run many LDAs
# with different k's
# Examine the topics  --------------
head(lda_topics)
# turn our LDA into the
# tidy format of the tidyverse packages
lda_tidy <- tidy(lda)
# Beta shows how much each term
# contributes to each topic
# NOTE: Typically list 8 terms when topic modelling
top_terms <- lda_tidy %>%
group_by(topic) %>%
top_n(8, beta) %>%
ungroup() %>%
arrange(topic, -beta)
head(top_terms, 10)
# visualize with ggplot
# NOTE: We want to treat the topics as categorical,
# so turn it to a factor first.
# NOTE: Don't show legend because it's not helpful.
top_terms %>%
mutate(term = reorder(term, beta)) %>%
ggplot(aes(term,
beta,
fill = factor(topic))) +
geom_col(show.legend=FALSE) +
facet_wrap( ~ topic, scales="free") +
coord_flip()
k <- 4
# NOTE: Optimal k varies, so you'll need to experiment
# with which k value is best.
lda <- LDA(dtm,
k = k,
control = list(seed=42))
lda
# NOTE: seed keeps randomization consistent
# get the words out of our LDA
lda_words <- terms(lda, 5)
lda_words
# Save LDA words as CSV --------------
# need a matrix format to be written to csv
lda_topics <- as.matrix(lda_words)
# write.csv(lda_topics,
#           file = paste("(new)LDA_",
#                        k,
#                        ".csv",
#                        sep = ""))
# NOTE: k variable is helpful here when you run many LDAs
# with different k's
# Examine the topics  --------------
head(lda_topics)
# turn our LDA into the
# tidy format of the tidyverse packages
lda_tidy <- tidy(lda)
# Beta shows how much each term
# contributes to each topic
# NOTE: Typically list 8 terms when topic modelling
top_terms <- lda_tidy %>%
group_by(topic) %>%
top_n(8, beta) %>%
ungroup() %>%
arrange(topic, -beta)
head(top_terms, 10)
# Export to CSV
#
# write.csv(top_terms,
#           file = paste("(new)01-LDA_",
#                        k,
#                        ".csv",
#                        sep = ""))
#
# visualize with ggplot
# NOTE: We want to treat the topics as categorical,
# so turn it to a factor first.
# NOTE: Don't show legend because it's not helpful.
top_terms %>%
mutate(term = reorder(term, beta)) %>%
ggplot(aes(term,
beta,
fill = factor(topic))) +
geom_col(show.legend=FALSE) +
facet_wrap( ~ topic, scales="free") +
coord_flip()
