###################################################
# IMPORT LIBRARIES
#to create and work with corpora
library(tm)
#for LDA topic models
library(topicmodels)
# other useful packages
library(tidyverse)
library(tidytext)
library(stringr)
# dplyr must be placed below data.table
library(data.table)
library(dplyr)
###################################################
# SETUP DIRECTORY
# Clear R's memory.
rm(list=ls())
# Set working directory
setwd("C:/Users/Oscar Ko/Desktop/100-Days-of-Data-Projects/19-NLP_Subreddit_AsianAmerican")
# Check current working directory
getwd()
# Lists files in current working directory
list.files()
###################################################
# IMPORT DATA
# Faster way to import CSV with "vroom" package
library(vroom)
comments <- vroom("data/AsianAmerican_comments.csv")
posts <- vroom("data/AsianAmerican_posts.csv")
###################################################
# Combine comment text with original post
colnames(comments)
colnames(posts)
# Join dfs on post_id and id
# keep comment body
# Keep post title (& text if exists)
df <- data.frame(matrix(ncol = 2, nrow = 0))
column_names <- c("post_id", "post_and_comments")
colnames(df) <- columns
for (id in posts$id) {
post_title = c(posts[posts$id == id,]$title)
all_comments = comments[comments$post_id == id,]$body
all_text_vector = c(post_title, all_comments)
text_string = paste(all_text_vector, collapse = ' ')
row = data.frame(
post_id = id,
post_and_comments = text_string
)
df = rbind(df, row)
}
# check number of rows and columns
nrow(data)
# check number of rows and columns
nrow(df)
ncol(df)
# check column names
colnames(df)
# check for missing values
any(is.na(df))
str(df)
summary(df)
head(df)
